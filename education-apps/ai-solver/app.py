import streamlit as st
import google.generativeai as genai
from PIL import Image
import os

# --- Config ---
st.set_page_config(page_title="AI Step-by-Step Solver", page_icon="ğŸ“")
st.title("ğŸ“ AI Step-by-Step Solver (AI è§£é¡Œå°å¸«)")

# API Key Handling (You can replace this with st.secrets)
api_key = st.sidebar.text_input("Enter Gemini API Key", type="password")
if not api_key:
    st.info("è«‹è¼¸å…¥ API Key ä»¥é–‹å§‹ä½¿ç”¨ã€‚")
    st.stop()

genai.configure(api_key=api_key)
model = genai.GenerativeModel('gemini-1.5-pro') # Using multimodal model

# --- System Prompt ---
SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€ä½æ“æœ‰è˜‡æ ¼æ‹‰åº•å¼æ•™å­¸æ³•çš„é ‚å°–æ•¸å­¸/ç§‘å­¸å°å¸«ã€‚
ç•¶å­¸ç”Ÿä¸Šå‚³é¡Œç›®æˆ–ç™¼å•æ™‚ï¼Œè«‹éµå¾ªä»¥ä¸‹åŸå‰‡ï¼š
1. **çµ•ä¸ç›´æ¥çµ¦å‡ºç­”æ¡ˆ**ã€‚
2. **å¼•å°æ€è€ƒ**ï¼šæå‡ºé—œéµå•é¡Œï¼Œè®“å­¸ç”Ÿè‡ªå·±ç™¼ç¾ä¸‹ä¸€æ­¥ã€‚ä¾‹å¦‚ï¼šã€Œé€™é¡Œçœ‹èµ·ä¾†åƒæ˜¯ä¸€å…ƒäºŒæ¬¡æ–¹ç¨‹å¼ï¼Œä½ è¨˜å¾—ç¬¬ä¸€æ­¥é€šå¸¸è¦åšä»€éº¼å—ï¼Ÿã€
3. **éŒ¯èª¤åµæ¸¬**ï¼šå¦‚æœå­¸ç”Ÿç®—å‡ºéŒ¯èª¤ç­”æ¡ˆï¼Œè«‹æŒ‡å‡ºå…·é«”çš„é‚è¼¯æ¼æ´ï¼ˆä¾‹å¦‚ï¼šã€Œä½ çš„è² è™Ÿæ˜¯ä¸æ˜¯åœ¨ç§»é …æ™‚å¿˜è¨˜è®Šè™Ÿäº†ï¼Ÿã€ï¼‰ã€‚
4. **èªæ°£æº«æŸ”ä¸”é¼“å‹µ**ï¼šåƒä¸€ä½æœ‰è€å¿ƒçš„å®¶æ•™è€å¸«ã€‚
5. **åˆ†æ­¥é©Ÿ**ï¼šä¸€æ¬¡åªå¼•å°ä¸€æ­¥ï¼Œä¸è¦ä¸€æ¬¡è¬›å®Œæ‰€æœ‰æ¦‚å¿µã€‚
"""

# --- Chat History ---
if "messages" not in st.session_state:
    st.session_state.messages = []
    # Add system prompt as context (hidden from UI, but sent to model)
    # Note: Streamlit chat history usually stores displayable messages. 
    # We'll prepend the system prompt logic in the API call.

# --- UI ---
with st.sidebar:
    st.header("ä¸Šå‚³é¡Œç›®")
    uploaded_file = st.file_uploader("æ‹ç…§æˆ–ä¸Šå‚³åœ–ç‰‡", type=["jpg", "png", "jpeg"])
    image = None
    if uploaded_file:
        image = Image.open(uploaded_file)
        st.image(image, caption="é¡Œç›®é è¦½", use_column_width=True)

# Display Chat
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.write(msg["content"])
        if "image" in msg and msg["image"]:
            st.image(msg["image"], width=200)

# Input
user_input = st.chat_input("è¼¸å…¥ä½ çš„å•é¡Œæˆ–æ˜¯ç®—å¼...")

if user_input or (image and len(st.session_state.messages) == 0):
    # Handle initial image upload trigger or text input
    prompt_parts = [SYSTEM_PROMPT]
    
    # Add history
    for msg in st.session_state.messages:
        prompt_parts.append(f"{msg['role']}: {msg['content']}")
    
    # Current input
    user_msg_content = user_input if user_input else "è«‹å¹«æˆ‘çœ‹çœ‹é€™é¡Œæ€éº¼åšï¼Ÿ"
    prompt_parts.append(f"user: {user_msg_content}")
    
    # Display User Message
    with st.chat_message("user"):
        st.write(user_msg_content)
        if image and len(st.session_state.messages) == 0:
            st.image(image, width=200)
    
    st.session_state.messages.append({"role": "user", "content": user_msg_content, "image": image if len(st.session_state.messages) == 0 else None})

    # Call AI
    with st.chat_message("assistant"):
        with st.spinner("æ€è€ƒä¸­..."):
            try:
                inputs = [user_msg_content]
                if image and len(st.session_state.messages) == 1: # Only send image on first turn or if strictly needed
                     inputs.append(image)
                
                # Full prompt construction for simple chat (stateless API usage for demo)
                # Ideally use chat = model.start_chat()
                
                chat = model.start_chat(history=[])
                # We need to inject system prompt behavior. Gemini supports system instructions in newer versions or via prompt.
                # For simplicity in this demo, we prepend text.
                
                if image:
                    response = model.generate_content([SYSTEM_PROMPT, image, user_msg_content])
                else:
                    # Construct history for text-only context if needed, but start_chat handles it better.
                    # Simplified for "one-shot" feel + history context in prompt if not using chat session object
                    # Let's use specific prompt construction:
                    full_prompt = [SYSTEM_PROMPT]
                    for m in st.session_state.messages:
                        if m["image"]: full_prompt.append(m["image"])
                        full_prompt.append(f"{m['role']}: {m['content']}")
                    
                    response = model.generate_content(full_prompt)

                st.write(response.text)
                st.session_state.messages.append({"role": "assistant", "content": response.text})
            except Exception as e:
                st.error(f"ç™¼ç”ŸéŒ¯èª¤: {e}")
